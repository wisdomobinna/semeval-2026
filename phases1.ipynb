{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22948,
     "status": "ok",
     "timestamp": 1762280864876,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "piznh_HChXG4",
    "outputId": "b97be53c-c72b-4c5f-c943-9ef47263ab1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q anthropic openai pandas numpy tqdm datasets scikit-learn\n",
    "!pip install google-generativeai together -q\n",
    "!pip install cohere tenacity -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1762282472288,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "tYNFmqq1hdV7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from google.colab import drive\n",
    "from datetime import datetime\n",
    "import anthropic\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import cohere\n",
    "from together import Together\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27489,
     "status": "ok",
     "timestamp": 1762281515331,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "lQakTK_jhdMG",
    "outputId": "242dc566-3f15-4715-b063-eaf9e4316c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/PhD/Courses/year_2/text_analytics/POLAR_SemEval2026\"\n",
    "DATA_DIR = f\"{PROJECT_ROOT}/data\"\n",
    "OUTPUT_DIR = f\"{PROJECT_ROOT}/tier1_output\"\n",
    "PHASE1_OUTPUT = f\"{OUTPUT_DIR}/phase1_multitemp\"\n",
    "os.makedirs(PHASE1_OUTPUT, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1762284959893,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "oI2hItTEhdTs"
   },
   "outputs": [],
   "source": [
    "# API keys\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"\"\n",
    "# os.environ[\"GEMINI_API_KEY\"] = xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "as0K9qs8hdP_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1762284962108,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "30DJinezhhUZ"
   },
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "claude_client = anthropic.Anthropic()\n",
    "openai_client = OpenAI()\n",
    "deepseek_client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=\"https://api.deepseek.com\")\n",
    "# genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "gemini_client = gemini_client = genai.GenerativeModel('gemini-1.5-pro')\n",
    "cohere_client = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "together_client = Together()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21LBR16AhhRz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1762284964217,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "u7ozyKVbhlxM"
   },
   "outputs": [],
   "source": [
    "# Languages\n",
    "LANGUAGES = ['zho', 'tur', 'eng', 'arb', 'hin']\n",
    "LANGUAGE_NAMES = {'zho': 'Chinese', 'tur': 'Turkish', 'eng': 'English', 'arb': 'Arabic', 'hin': 'Hindi'}\n",
    "\n",
    "# Temperatures to test\n",
    "TEMPERATURES = [0.0, 0.5, 1.0]\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert at detecting online polarization.\n",
    "\n",
    "POLARIZATION is content that creates sharp division into opposing groups with these characteristics:\n",
    "\n",
    "POLARIZED content has:\n",
    "- Us vs. Them framing: \"our people\" vs \"those people\", in-group vs out-group language\n",
    "- Hostility or contempt toward a group (political, religious, racial, gender, etc.)\n",
    "- Moral condemnation: portraying one side as evil/corrupt/dangerous\n",
    "- Stereotyping or vilification of entire groups\n",
    "- Dehumanizing language\n",
    "- Zero-sum framing: if they win, we lose\n",
    "\n",
    "NOT polarized:\n",
    "- Factual reporting or neutral information\n",
    "- Policy disagreement without hostility\n",
    "- Criticism of specific actions/individuals without group-level attacks\n",
    "- Personal opinions without divisive us-vs-them framing\n",
    "\n",
    "Respond ONLY with JSON:\n",
    "{\n",
    "  \"is_polarized\": true/false,\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"reasoning\": \"brief explanation\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrfiprWTumzA"
   },
   "outputs": [],
   "source": [
    "def call_model_at_temp(text, language_name, model_name, temperature):\n",
    "    \"\"\"Call model at specific temperature\"\"\"\n",
    "    try:\n",
    "        if model_name == 'DeepSeek':\n",
    "            response = deepseek_client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": f\"Language: {language_name}\\n\\nText: {text}\\n\\nAnalyze:\"}\n",
    "                ]\n",
    "            )\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            if response_text.startswith('```'):\n",
    "                lines = response_text.split('\\n')[1:]\n",
    "                if lines[-1].strip() == '```':\n",
    "                    lines = lines[:-1]\n",
    "                response_text = '\\n'.join(lines).strip()\n",
    "            response = json.loads(response_text)\n",
    "\n",
    "        elif model_name == 'GPT-4o':\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=temperature,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                timeout=30.0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": f\"Language: {language_name}\\n\\nText: {text}\\n\\nAnalyze:\"}\n",
    "                ]\n",
    "            )\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            response = json.loads(response_text)\n",
    "\n",
    "        elif model_name == 'Qwen2.5':\n",
    "            response = together_client.chat.completions.create(\n",
    "                model=\"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": f\"Language: {language_name}\\n\\nText: {text}\\n\\nAnalyze:\"}\n",
    "                ]\n",
    "            )\n",
    "            response = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        # Extract prediction\n",
    "        predicted_val = response.get('is_polarized', None)\n",
    "        if isinstance(predicted_val, bool):\n",
    "            predicted_val = 1 if predicted_val else 0\n",
    "\n",
    "        return {\n",
    "            'prediction': predicted_val,\n",
    "            'confidence': response.get('confidence', None),\n",
    "            'error': None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'prediction': None, 'confidence': None, 'error': str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9yGakiYumpb"
   },
   "outputs": [],
   "source": [
    "def calculate_variance_confidence(predictions):\n",
    "    \"\"\"Calculate confidence from prediction variance\"\"\"\n",
    "    valid_preds = [p for p in predictions if p is not None]\n",
    "    if not valid_preds:\n",
    "        return None, 0.0, True\n",
    "\n",
    "    # Agreement: majority vote strength\n",
    "    counts = Counter(valid_preds)\n",
    "    majority_pred = counts.most_common(1)[0][0]\n",
    "    agreement = counts[majority_pred] / len(valid_preds)\n",
    "\n",
    "    # Variance-based confidence: high agreement = high confidence\n",
    "    variance_conf = agreement\n",
    "\n",
    "    # Flag as hard if agreement < 67% (2 out of 3)\n",
    "    is_hard = agreement < 0.67\n",
    "\n",
    "    return majority_pred, variance_conf, is_hard\n",
    "\n",
    "def process_sample(row, models_to_use):\n",
    "    \"\"\"Process one sample through specified models and temperatures\"\"\"\n",
    "    results = {\n",
    "        'id': row['id'],\n",
    "        'text': row['text'],\n",
    "        'language': row['language'],\n",
    "        'language_name': row['language_name'],\n",
    "        'true_label': int(row['polarization'])\n",
    "    }\n",
    "\n",
    "    all_predictions = []\n",
    "    all_confidences = []\n",
    "\n",
    "    # Get predictions for each model at each temperature\n",
    "    for model_name in models_to_use:\n",
    "        model_preds = []\n",
    "        model_confs = []\n",
    "\n",
    "        for temp in TEMPERATURES:\n",
    "            pred = call_model_at_temp(row['text'], row['language_name'], model_name, temp)\n",
    "\n",
    "            # Store individual prediction\n",
    "            results[f'{model_name}_t{temp}_pred'] = pred['prediction']\n",
    "            results[f'{model_name}_t{temp}_conf'] = pred['confidence']\n",
    "            results[f'{model_name}_t{temp}_error'] = pred['error']\n",
    "\n",
    "            if pred['prediction'] is not None:\n",
    "                model_preds.append(pred['prediction'])\n",
    "                all_predictions.append(pred['prediction'])\n",
    "            if pred['confidence'] is not None:\n",
    "                model_confs.append(pred['confidence'])\n",
    "                all_confidences.append(pred['confidence'])\n",
    "\n",
    "        # Per-model majority and confidence\n",
    "        maj_pred, maj_conf, is_uncertain = calculate_variance_confidence(model_preds)\n",
    "        results[f'{model_name}_majority'] = maj_pred\n",
    "        results[f'{model_name}_var_confidence'] = maj_conf\n",
    "        results[f'{model_name}_uncertain'] = is_uncertain\n",
    "        results[f'{model_name}_avg_confidence'] = np.mean(model_confs) if model_confs else None\n",
    "\n",
    "    # Only calculate ensemble if we have all models\n",
    "    if len(models_to_use) == 3:\n",
    "        ensemble_pred, ensemble_conf, is_hard = calculate_variance_confidence(all_predictions)\n",
    "        results['ensemble_prediction'] = ensemble_pred\n",
    "        results['ensemble_confidence'] = ensemble_conf\n",
    "        results['is_hard_negative'] = is_hard\n",
    "        results['avg_model_confidence'] = np.mean(all_confidences) if all_confidences else None\n",
    "\n",
    "    return results\n",
    "\n",
    "def load_full_train_data():\n",
    "    \"\"\"Load all training samples\"\"\"\n",
    "    all_data = []\n",
    "    for lang in LANGUAGES:\n",
    "        df = pd.read_csv(f\"{DATA_DIR}/subtask1/train/{lang}.csv\")\n",
    "        df['language'] = lang\n",
    "        df['language_name'] = LANGUAGE_NAMES[lang]\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "def load_existing_results(model_name):\n",
    "    \"\"\"\n",
    "    Load existing results from checkpoint or final file\n",
    "    Returns: (results_df, last_processed_idx)\n",
    "    \"\"\"\n",
    "    output_path = f\"{PHASE1_OUTPUT}/{model_name}_predictions.csv\"\n",
    "\n",
    "    # Check for final output first\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Found existing results file: {output_path}\")\n",
    "        df = pd.read_csv(output_path)\n",
    "        return df, len(df)\n",
    "\n",
    "    # Look for most recent checkpoint\n",
    "    checkpoint_files = [f for f in os.listdir(PHASE1_OUTPUT)\n",
    "                       if f.startswith(f\"{model_name}_checkpoint_\") and f.endswith('.csv')]\n",
    "\n",
    "    if checkpoint_files:\n",
    "        # Get the most recent checkpoint (highest number)\n",
    "        checkpoint_nums = [int(f.split('_')[-1].replace('.csv', '')) for f in checkpoint_files]\n",
    "        latest_checkpoint_num = max(checkpoint_nums)\n",
    "        latest_checkpoint = f\"{PHASE1_OUTPUT}/{model_name}_checkpoint_{latest_checkpoint_num}.csv\"\n",
    "\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "        df = pd.read_csv(latest_checkpoint)\n",
    "        return df, len(df)\n",
    "\n",
    "    return None, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP1NWg2YurfO"
   },
   "outputs": [],
   "source": [
    "def load_full_train_data():\n",
    "    \"\"\"Load all training samples\"\"\"\n",
    "    all_data = []\n",
    "    for lang in LANGUAGES:\n",
    "        df = pd.read_csv(f\"{DATA_DIR}/subtask1/train/{lang}.csv\")\n",
    "        df['language'] = lang\n",
    "        df['language_name'] = LANGUAGE_NAMES[lang]\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKUnjbYV6gpj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhrCAxvK6gis"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_model(model_name, sample_size=None, start_idx=0, checkpoint_every=50, force_restart=False):\n",
    "    \"\"\"\n",
    "    Run predictions for a single model across all temperatures\n",
    "\n",
    "    Args:\n",
    "        model_name: 'DeepSeek', 'GPT-4o', or 'Qwen2.5'\n",
    "        sample_size: Number of samples to process (None = all)\n",
    "        start_idx: Starting index for resuming runs (overridden by existing results)\n",
    "        checkpoint_every: Save checkpoint every N samples\n",
    "        force_restart: If True, ignore existing results and start from scratch\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load full data\n",
    "    df = load_full_train_data()\n",
    "    total_samples = len(df)\n",
    "\n",
    "    # Check for existing results unless force_restart\n",
    "    existing_results = None\n",
    "    resume_from_idx = start_idx\n",
    "\n",
    "    if not force_restart:\n",
    "        existing_results, processed_count = load_existing_results(model_name)\n",
    "        if existing_results is not None and processed_count > 0:\n",
    "            resume_from_idx = processed_count\n",
    "            print(f\"Resuming from index {resume_from_idx} ({processed_count} samples already processed)\")\n",
    "            print(f\"Progress: {processed_count}/{total_samples} ({processed_count/total_samples*100:.1f}%)\")\n",
    "\n",
    "            # Ask user if they want to continue\n",
    "            user_input = input(f\"\\nContinue from index {resume_from_idx}? (y/n): \").strip().lower()\n",
    "            if user_input != 'y':\n",
    "                print(\"Starting fresh...\")\n",
    "                existing_results = None\n",
    "                resume_from_idx = start_idx\n",
    "\n",
    "    # Apply sampling if requested\n",
    "    if sample_size is not None:\n",
    "        end_idx = min(resume_from_idx + sample_size, total_samples)\n",
    "        df_to_process = df.iloc[resume_from_idx:end_idx].copy()\n",
    "    else:\n",
    "        df_to_process = df.iloc[resume_from_idx:].copy()\n",
    "\n",
    "    # Reset index for clean processing\n",
    "    df_to_process = df_to_process.reset_index(drop=True)\n",
    "\n",
    "    # Initialize results storage\n",
    "    if existing_results is not None:\n",
    "        results = existing_results.to_dict('records')\n",
    "    else:\n",
    "        results = []\n",
    "\n",
    "    error_count = 0\n",
    "\n",
    "    print(f\"\\nProcessing {len(df_to_process)} samples...\")\n",
    "    print(f\"Checkpoints will be saved every {checkpoint_every} samples\")\n",
    "\n",
    "    # Process each sample\n",
    "    for idx in tqdm(range(len(df_to_process)), desc=f\"{model_name}\"):\n",
    "        row = df_to_process.iloc[idx]\n",
    "        actual_idx = resume_from_idx + idx  # Track actual position in full dataset\n",
    "\n",
    "        try:\n",
    "            # Process sample with this model only\n",
    "            result = process_sample(row, models_to_use=[model_name])\n",
    "            results.append(result)\n",
    "\n",
    "            # Track errors\n",
    "            has_error = any(\n",
    "                result.get(f'{model_name}_t{temp}_error') is not None\n",
    "                for temp in TEMPERATURES\n",
    "            )\n",
    "            if has_error:\n",
    "                error_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"\\nError at index {actual_idx}: {str(e)}\")\n",
    "            # Create empty result with error\n",
    "            results.append({\n",
    "                'id': row['id'],\n",
    "                'text': row['text'],\n",
    "                'language': row['language'],\n",
    "                'language_name': row['language_name'],\n",
    "                'true_label': int(row['polarization']),\n",
    "                f'{model_name}_majority': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (idx + 1) % checkpoint_every == 0:\n",
    "            checkpoint_df = pd.DataFrame(results)\n",
    "            checkpoint_path = f\"{PHASE1_OUTPUT}/{model_name}_checkpoint_{len(results)}.csv\"\n",
    "            checkpoint_df.to_csv(checkpoint_path, index=False)\n",
    "            print(f\"\\nCheckpoint saved: {checkpoint_path} ({len(results)}/{total_samples} samples)\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save final results\n",
    "    output_path = f\"{PHASE1_OUTPUT}/{model_name}_predictions.csv\"\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Clean up old checkpoints\n",
    "    checkpoint_files = [f for f in os.listdir(PHASE1_OUTPUT)\n",
    "                       if f.startswith(f\"{model_name}_checkpoint_\") and f.endswith('.csv')]\n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        try:\n",
    "            os.remove(f\"{PHASE1_OUTPUT}/{checkpoint_file}\")\n",
    "            print(f\"Removed old checkpoint: {checkpoint_file}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Calculate metrics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - Results Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Get valid predictions\n",
    "    valid_mask = results_df[f'{model_name}_majority'].notna()\n",
    "    valid_results = results_df[valid_mask]\n",
    "\n",
    "    if len(valid_results) > 0:\n",
    "        y_true = valid_results['true_label']\n",
    "        y_pred = valid_results[f'{model_name}_majority']\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Total samples: {len(results_df)}\")\n",
    "        print(f\"Valid predictions: {len(valid_results)} ({len(valid_results)/len(results_df)*100:.1f}%)\")\n",
    "        print(f\"Errors: {error_count}\")\n",
    "        print(f\"\\nPerformance Metrics:\")\n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "\n",
    "        # Per-temperature metrics\n",
    "        print(f\"\\nPer-Temperature Performance:\")\n",
    "        for temp in TEMPERATURES:\n",
    "            temp_valid = results_df[f'{model_name}_t{temp}_pred'].notna()\n",
    "            if temp_valid.sum() > 0:\n",
    "                temp_results = results_df[temp_valid]\n",
    "                temp_acc = accuracy_score(\n",
    "                    temp_results['true_label'],\n",
    "                    temp_results[f'{model_name}_t{temp}_pred']\n",
    "                )\n",
    "                temp_f1 = f1_score(\n",
    "                    temp_results['true_label'],\n",
    "                    temp_results[f'{model_name}_t{temp}_pred']\n",
    "                )\n",
    "                print(f\"  T={temp}: Acc={temp_acc:.4f}, F1={temp_f1:.4f}\")\n",
    "\n",
    "        # Confidence analysis\n",
    "        avg_var_conf = valid_results[f'{model_name}_var_confidence'].mean()\n",
    "        avg_model_conf = valid_results[f'{model_name}_avg_confidence'].mean()\n",
    "        uncertain_pct = valid_results[f'{model_name}_uncertain'].sum() / len(valid_results) * 100\n",
    "\n",
    "        print(f\"\\nConfidence Analysis:\")\n",
    "        print(f\"  Avg variance confidence: {avg_var_conf:.4f}\")\n",
    "        print(f\"  Avg model confidence: {avg_model_conf:.4f}\")\n",
    "        print(f\"  Uncertain samples: {uncertain_pct:.1f}%\")\n",
    "\n",
    "        # Per-language breakdown\n",
    "        print(f\"\\nPer-Language Performance:\")\n",
    "        for lang in valid_results['language'].unique():\n",
    "            lang_data = valid_results[valid_results['language'] == lang]\n",
    "            if len(lang_data) > 0:\n",
    "                lang_acc = accuracy_score(\n",
    "                    lang_data['true_label'],\n",
    "                    lang_data[f'{model_name}_majority']\n",
    "                )\n",
    "                lang_f1 = f1_score(\n",
    "                    lang_data['true_label'],\n",
    "                    lang_data[f'{model_name}_majority']\n",
    "                )\n",
    "                print(f\"  {lang}: N={len(lang_data)}, Acc={lang_acc:.4f}, F1={lang_f1:.4f}\")\n",
    "    else:\n",
    "        print(f\"WARNING: No valid predictions obtained!\")\n",
    "        print(f\"Total samples: {len(results_df)}\")\n",
    "        print(f\"Errors: {error_count}\")\n",
    "\n",
    "    print(f\"\\nResults saved to: {output_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsScOADQ6gbG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "error",
     "timestamp": 1762280837738,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "d2IDR8KFurdD",
    "outputId": "ccff9bc4-3b2f-4e74-edc8-a5eeb00c0744"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4110839362.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DeepSeek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# run_model('GPT-4o')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# run_model('Qwen2.5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# combine_results()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_model' is not defined"
     ]
    }
   ],
   "source": [
    "run_model('DeepSeek')\n",
    "# run_model('GPT-4o')\n",
    "# run_model('Qwen2.5')\n",
    "# combine_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee7A3jTfzPpU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1762280098862,
     "user": {
      "displayName": "Wisdom Obinna",
      "userId": "12263021393935922816"
     },
     "user_tz": 300
    },
    "id": "ALGpE8DqzPm4",
    "outputId": "ed26dc2f-f236-4d61-cdf8-ebe3b2b8bc0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mQlCGIuu0wX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrBIc7nNFrdj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqLk3bmOJZwnzylxxR/UY7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
